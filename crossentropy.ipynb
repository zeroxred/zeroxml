{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "#### Example and explantion based on:\n##### https://stackoverflow.com/questions/41990250/what-is-cross-entropy"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Cross-entropy\n#### Is commonly used to quantify the difference between two probability distributions. \n#### Usually the \"true\" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution."}, {"source": "import numpy as np", "metadata": {"collapsed": true}, "execution_count": 24, "outputs": [], "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "#### For example, suppose for a specific training instance, the label is B (out of the possible labels A, B, and C). \n#### The one-hot distribution for this training instance is therefore:\n\n#### Pr(Class A)  Pr(Class B)  Pr(Class C)\n####      [   0.0,          1.0,          0.0]\n\n"}, {"source": "p = np.array([0,1,0])\nprint(\"\\np:=\",p)", "metadata": {"collapsed": false, "scrolled": true}, "execution_count": 25, "outputs": [{"text": "\np:= [0 1 0]\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "#### You can interpret the above \"true\" distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C.\n\n#### Now, suppose your machine learning algorithm predicts the following probability distribution:\n\n#### Pr(Class A)  Pr(Class B)  Pr(Class C)\n####       [0.228,        0.619,        0.153]"}, {"source": "q = np.array([0.228,0.619,0.153])\nprint(\"\\nq:=\",q)", "metadata": {"collapsed": false}, "execution_count": 26, "outputs": [{"text": "\nq:= [ 0.228  0.619  0.153]\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "#### How close is the predicted distribution to the true distribution? That is what the cross-entropy loss determines. \n\n#### Use this formula:\n\n$$H(p,q) = -  \\sum\\limits_{i=0}^n p(i) \\log q(i)$$\n\n#### The sum is over the three classes A, B, and C, vector p is the correct label while q is our prediction as said before.\n#### Since log can trought a negative number we multiply by -1 to avoid this problem, also we need to be vary carefull since H(p,q) is not the same as H(q,t) because the correct vector can have zeros we dont want to calculate log of zeros(usually the predict output is based on a softmax function).\n####  If you complete the calculation, you will find that the loss is 0.479. So that is how \"wrong\" or \"far away\" your prediction is from the true distribution."}, {"source": "loss = - np.sum(p * np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 27, "outputs": [{"text": "\nloss:= 0.479650006298\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "#### We can also use dot product to avoid using sumation on code and math\n$$H(p,q) = - p(i) \\log q(i)$$"}, {"source": "# Using dot product\nloss = - p.dot(np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 28, "outputs": [{"text": "\nloss:= 0.479650006298\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "# Lets say we predicted almost corrected\n# since we cannot take log of zeros, and the output function in our model commonly being softmax function to avoid this problem.\n\nq = np.array([0.1,.8,0.1])\nprint(\"\\nq:=\",q)", "metadata": {"collapsed": false}, "execution_count": 29, "outputs": [{"text": "\nq:= [ 0.1  0.8  0.1]\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "loss = - np.sum(p * np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 30, "outputs": [{"text": "\nloss:= 0.223143551314\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "# Using dot product\nloss = - p.dot(np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 31, "outputs": [{"text": "\nloss:= 0.223143551314\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "# And we go even better\n\nq = np.array([0.001,0.998,0.001])\nprint(\"\\nq:=\",q)", "metadata": {"collapsed": false}, "execution_count": 32, "outputs": [{"text": "\nq:= [ 0.001  0.998  0.001]\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "loss = - np.sum(p * np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 33, "outputs": [{"text": "\nloss:= 0.00200200267067\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"source": "# Using dot product\nloss = - p.dot(np.log(q))\nprint(\"\\nloss:=\",loss)", "metadata": {"collapsed": false}, "execution_count": 34, "outputs": [{"text": "\nloss:= 0.00200200267067\n", "output_type": "stream", "name": "stdout"}], "cell_type": "code"}, {"metadata": {}, "cell_type": "markdown", "source": "#### In conclusion Cross entropy take two probability distribution(correct, prediction) and mesure the error between them, Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss). "}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python", "version": "3.4.5", "file_extension": ".py"}}, "nbformat_minor": 0}